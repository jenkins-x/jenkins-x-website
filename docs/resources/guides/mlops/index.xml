<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jenkins X - Cloud Native CI/CD Built On Kubernetes â€“ MLOps with Jenkins X</title>
    <link>https://jenkins-x.io/docs/resources/guides/mlops/</link>
    <description>Recent content in MLOps with Jenkins X on Jenkins X - Cloud Native CI/CD Built On Kubernetes</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 13 Mar 2020 14:57:07 +0000</lastBuildDate>
    
	  <atom:link href="https://jenkins-x.io/docs/resources/guides/mlops/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Introduction</title>
      <link>https://jenkins-x.io/docs/resources/guides/mlops/introduction/</link>
      <pubDate>Fri, 13 Mar 2020 15:03:05 +0000</pubDate>
      
      <guid>https://jenkins-x.io/docs/resources/guides/mlops/introduction/</guid>
      <description>
        
        
        &lt;h2 id=&#34;understanding-mlops&#34;&gt;Understanding MLOps&lt;/h2&gt;
&lt;p&gt;When we talk about MLOps, what we are referring to is best defined as the extension of the DevOps methodology to include Machine Learning and Data Science assets as first class citizens within the best known methods of DevOps. The aim is to demystify Machine Learning and re-integrate the delivery of technology solutions as a seamless process of product development and deployment.&lt;/p&gt;
&lt;p&gt;The Continuous Delivery Foundation hosts a collaborative Roadmap for MLOps and Jenkins X is committed to implementing features aligned to that model.&lt;/p&gt;
&lt;p&gt;Software systems including Machine Learning components tend to be decision-making systems rather than just data processing systems and thus are required to be held accountable to much higher standards than most conventional software projects. As a result, it is especially important that release processes associated with these solutions meet the highest levels of quality.&lt;/p&gt;
&lt;p&gt;Machine Learning project teams are however often a relatively new addition to many organisations and are sometimes lacking in experience when it comes to delivering solutions into production. MLOps is an approach designed to help bridge the gap between where teams find themselves today and where they need to reach in terms of quality and maturity.&lt;/p&gt;
&lt;p&gt;Organisations pursuing the development of ML products need to be able to manage cost and time to market by optimising the process of taking ML features into production, reducing Lead Time and shortening the feedback loop between production and development for ML assets.&lt;/p&gt;
&lt;p&gt;With Jenkins X, we unify the release cycle for ML and conventional software assets, allowing you to manage your whole solution as part of one standard release process. By doing this, you are able to apply conventional automated testing approaches to your ML assets as well as the rest of your codebase, extending security checks, static analysis, dynamic analysis, dependency scanning and integrity checking to your entire application stack.&lt;/p&gt;
&lt;p&gt;MLOps is a framework-agnostic methodology and Jenkins X supports the deployment of ML assets based upon a wide range of different ML frameworks and libraries. We currently support training and deployment of models using both CPU and GPU, across a range of Cloud infrastructure, or in-house.&lt;/p&gt;
&lt;p&gt;The use of MLOps should teach best known methods of applying MLOps. It should be recognised that many customers will be experts in the field of Data Science but may have had relatively little exposure to DevOps or other SDLC principles. To minimise this learning curve, the MLOps features of Jenkins X provide defaults aligned to best practice in production environments, reducing the amount of effort necessary to get working solutions into production reliably.&lt;/p&gt;
&lt;h2 id=&#34;what-about-my-jupyter-notebooks&#34;&gt;What about my Jupyter Notebooks?&lt;/h2&gt;
&lt;p&gt;It is common to teach the basics of Machine Learning using convenient scratchpad environments such as Jupyter Notebooks that facilitate casual experimentation and ease of learning. Such courses rarely however progress on to teaching more advanced topics regarding the challenges of managing real-life software assets in mission-critical situations. This leaves a significant knowledge gap across data science teams that must be filled before products can be safely delivered to customers.&lt;/p&gt;
&lt;p&gt;Jenkins X seeks to minimise the pain of moving teams beyond the &amp;rsquo;training-wheels&amp;rsquo; environment of scratch-built models, scripted ad-hoc on uncontrolled laptops.&lt;/p&gt;
&lt;p&gt;By providing a consistent, logical and familiar approach to managing all your project assets to the highest standards, it becomes simple to train teams to work collaboratively and effectively at pace.&lt;/p&gt;
&lt;p&gt;Simple &amp;lsquo;quickstarts&amp;rsquo; allow teams to create working example projects based upon best practice for managing critical IP that can be extended incrementally to deliver desired funtionality in a safe and friendly manner without relying upon everyone involved having in-depth knowledge of all the aspects of a robust software development lifecycle.&lt;/p&gt;
&lt;p&gt;When using MLOps features within Jenkins X, ML assets are automatically managed under version control and audit trails maintained such that it is always possible to understand which version of which data set, training script, model or service is deployed in any given production environment and to quickly revert or upgrade to specific versions of assets in the event of a problem.&lt;/p&gt;
&lt;p&gt;Jenkins X makes it easy to wrap model instances in service implementations so that they can be securely and robustly deployed into application solutions in a way that is resilient and scalable.&lt;/p&gt;
&lt;p&gt;Built-in release governance processes ensure that all ML assets pass through the same rigourous automated testing and QA processes normally applied to conventional software components, ensuring that you never have to worry about someone&amp;rsquo;s uncontrolled Jupyter Notebook making it into a customer-facing environment.&lt;/p&gt;
&lt;h2 id=&#34;breaking-down-the-new-wall&#34;&gt;Breaking down the New Wall&lt;/h2&gt;
&lt;p&gt;The success of DevOps as an approach is in large part due to the way in which it tore down the wall between software development teams designing new products and operational staff trying to own and run them cost-effectively and safely. Teaching developers to have responsibility for operational concerns and helping operational staff to communicate their requirements to development teams has been instrumental in reducing the time to market for new products.&lt;/p&gt;
&lt;p&gt;The introduction of Machine Learning capabilities through dedicated Data Science teams has created a new silo, a new wall to throw things over and a fresh chorus of &amp;ldquo;Well, it worked on my laptop!&amp;rdquo; in engineering departments worldwide.&lt;/p&gt;
&lt;p&gt;MLOps tears down this new wall to blur the line between Data Scientists, Software Developers, Operational teams and Product Owners, making it faster, safer and cheaper to get AI-driven ideas into the marketplace.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Using Machine Learning Quickstarts</title>
      <link>https://jenkins-x.io/docs/resources/guides/mlops/mlquickstarting/</link>
      <pubDate>Fri, 13 Mar 2020 15:03:05 +0000</pubDate>
      
      <guid>https://jenkins-x.io/docs/resources/guides/mlops/mlquickstarting/</guid>
      <description>
        
        
        &lt;p&gt;The Jenkins X MLOps Quickstarts Library provides template projects to make it quick and easy to set up everything you need to get started with a building a new ML-based asset.&lt;/p&gt;
&lt;p&gt;Each quickstart project comprises two repositories, one which contains your training script and a second which takes the final model you have trained and wraps it as a RESTful service for deployment into your overall solution.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;p&gt;You can create an instance of a project using the command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; jx create mlquickstart
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and follow the instructions to select a template from the list.&lt;/p&gt;
&lt;p&gt;Once the quickstart process completes, you will find two new projects in your current folder, one with the suffix &lt;code&gt;-training&lt;/code&gt; and the other with the suffix &lt;code&gt;-service&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The training project contains an example training script and some tests for the class of ML solution you selected. All the quickstarts are working examples, so you can see the solution in action and then start to modify it to meet your desired outcome.&lt;/p&gt;
&lt;p&gt;The service project is designed to take your model and make it easy to wrap it in a microservice so you can deploy it into your application. Notice that at this stage, there is no model in this service project because you haven&amp;rsquo;t trained it yet.&lt;/p&gt;
&lt;p&gt;If you check the Git account you used to create the quickstart project, you will see that the two folders have been created as repositories and linked to your Jenkins X cluster.&lt;/p&gt;
&lt;p&gt;Looking at your Jenkins X instance, you should be able to see that two builds have started for these projects. The service build will probably complete first, and it will create an instance of a new microservice in your staging environment, but this instance will fail to start and will end up in CrashLoopBackOff at this stage because it is waiting for the model to train.&lt;/p&gt;
&lt;p&gt;The second build is training an example model. When this completes successfully, it will persist the model in ONNX format and will make a Pull Request against your service project to add the model and the training metrics to your service. If you check the repository for the service, you should be able to see the open PR and can verify the files that have been added.&lt;/p&gt;
&lt;p&gt;At this stage, a preview environment will have been created for the service and you can verify its operation by connecting to it via a browser.&lt;/p&gt;
&lt;p&gt;Once you are happy with the model, merge the PR and the service will be redeployed into staging with the model you trained.&lt;/p&gt;
&lt;p&gt;You can watch this in action in the following video:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/AqL_ME7BM6U&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;customising-your-project&#34;&gt;Customising your project&lt;/h2&gt;
&lt;p&gt;Now you are ready to go back to the local copy of your training project and can start editing the training script to change the model.&lt;/p&gt;
&lt;p&gt;You will see that there are several steps annotated in the comments.&lt;/p&gt;
&lt;p&gt;The first step is where you define success criteria for your training. We only want to go to the effort of deploying an instance of our model if we consider it sufficiently accurate for our purposes so you should always create one or more metrics by which you will later judge whether your training run was successful.&lt;/p&gt;
&lt;p&gt;In the second step, we define the code to train your model. Feel free to change this to do what you want it to do.&lt;/p&gt;
&lt;p&gt;In the third step is where we evaluate the trained model we hold in memory against the criteria you specified earlier. You are free to modify this code in line with your desired metrics.&lt;/p&gt;
&lt;p&gt;Step four is only executed if your model passes the success threshold. If it does not, the training build is marked as &amp;lsquo;failed&amp;rsquo; and stops. Within this step, we convert the in-memory model into ONNX format and persist it temporarily to the local filesystem of the container in which the training build is running. We also write any metrics data and plots we wish to persist into a folder called &amp;lsquo;metrics&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;The training script exits at this point, however the Jenkins X pipeline for the training build will take the ONNX model and anything you placed in the &amp;lsquo;metrics&amp;rsquo; folder and will create a PR against the service repo as shown earlier.&lt;/p&gt;
&lt;h2 id=&#34;starting-a-training-run&#34;&gt;Starting a training run&lt;/h2&gt;
&lt;p&gt;To trigger a training run after modifying the training script, you need to commit your changes to the remote repository associated with this project:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; git add app.py
&amp;gt; git commit -m &amp;#34;feat: Added new training feature...&amp;#34;
&amp;gt; git push
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will trigger Jenkins X to start a new training run which you can monitor via the UI or with:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; jx get build logs
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;additional-training-runs&#34;&gt;Additional training runs&lt;/h2&gt;
&lt;p&gt;If you would like to trigger a training run to start again without modifying the script, perhaps because your initial run failed to meet your success criteria you can run:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; jx start pipeline
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and select the name of the training project you would like to trigger.&lt;/p&gt;
&lt;h2 id=&#34;versioned-assets&#34;&gt;Versioned assets&lt;/h2&gt;
&lt;p&gt;Every successful model trained creates a new version of your microservice. This enables you to do things like promoting an initial version of a model to your staging environment so that others on the team can focus on integrating your ML component with the rest of the application whilst you test alternate versions of the model in your preview environment to optimise performance. Once you are happy with your optimised model, you can promote it for integration by merging the Pull Request.&lt;/p&gt;
&lt;p&gt;You can also use the git repository to go back to previous model instances in the event that you need to investigate any issues that might occur with deployed versions of earlier code. This gives you full traceability and an audit trail for your models.&lt;/p&gt;
&lt;p&gt;Should you need to revert a model from a staging or production environment, you can simply change the revision number of the service application in the GitOps repo for the target environment to the last known good instance and commit your changes. Jenkins X will then update your environment as necessary.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Finding a Machine Learning Quickstart</title>
      <link>https://jenkins-x.io/docs/resources/guides/mlops/mlquickstarts/</link>
      <pubDate>Fri, 13 Mar 2020 15:03:05 +0000</pubDate>
      
      <guid>https://jenkins-x.io/docs/resources/guides/mlops/mlquickstarts/</guid>
      <description>
        
        
        &lt;p&gt;This directory is intended to help you find your way around the Jenkins X MLOps Quickstarts Library and get you up and running rapidly with a template project based around the class of Machine Learning approach you wish to work with and the language and framework you prefer.&lt;/p&gt;
&lt;p&gt;The directory is divided by target programming language (Python only at this stage, but with additional quickstarts to follow in other languages) and then by ML framework.&lt;/p&gt;
&lt;p&gt;The section for each framework is then divided by class of ML approach and CPU/GPU-based solutions.&lt;/p&gt;
&lt;p&gt;To create an instance of a project, find the title of the particular quickstart you wish to use and then select this from the list that is presented when you use the command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; jx create mlquickstart
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;python-quickstarts&#34;&gt;&lt;strong&gt;Python Quickstarts:&lt;/strong&gt;&lt;/h2&gt;
&lt;img src=&#34;https://avatars-04.gitter.im/group/iv/4/57542d4cc43b8c601977b6ad?s=48&#34; alt=&#34;LightGBM Logo&#34; width=&#34;40&#34; align=&#34;right&#34;&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lightgbm&#34;&gt;LightGBM&lt;/h2&gt;
&lt;p&gt;LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Faster training speed and higher efficiency.&lt;/li&gt;
&lt;li&gt;Lower memory usage.&lt;/li&gt;
&lt;li&gt;Better accuracy.&lt;/li&gt;
&lt;li&gt;Support of parallel and GPU learning.&lt;/li&gt;
&lt;li&gt;Capable of handling large-scale data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption.&lt;/p&gt;
&lt;p&gt;Documentation is at &lt;a href=&#34;https://lightgbm.readthedocs.io/&#34;&gt;https://lightgbm.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-lightgbm-cpu&lt;/code&gt; is a project for training and deploying tree based learning algorithms using the LightGBM library.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-lightgbm-cpu-training&#34;&gt;ML-python-lightgbm-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-lightgbm-cpu-service&#34;&gt;ML-python-lightgbm-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;img src=&#34;https://pytorch.org/assets/images/logo-dark.svg&#34; alt=&#34;PyTorch Logo&#34; width=&#34;140&#34; align=&#34;right&#34;&gt;
&lt;h2 id=&#34;pytorch&#34;&gt;PyTorch&lt;/h2&gt;
&lt;p&gt;Pytorch is a rich ecosystem of tools, libraries, and more to support, accelerate, and explore AI development.&lt;/p&gt;
&lt;p&gt;Documentation is at &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;convolutional-neural-networks&#34;&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-pytorch-cpu&lt;/code&gt; is a simple example demostrating the use of Pytorch with a Convolutional Neural Network (AlexNet) for image recognition.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-pytorch-cpu-training&#34;&gt;ML-python-pytorch-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-pytorch-cpu-service&#34;&gt;ML-python-pytorch-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;multi-layer-perceptron-networks&#34;&gt;Multi-layer Perceptron Networks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-pytorch-mlpc-cpu&lt;/code&gt; is a project for training and deploying Multi-layer Perceptron Networks in Pytorch.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-pytorch-mlpc-cpu-training&#34;&gt;ML-python-pytorch-mlpc-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-pytorch-mlpc-cpu-service&#34;&gt;ML-python-pytorch-mlpc-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-pytorch-mlpc-gpu&lt;/code&gt; is a project for training and deploying Multi-layer Perceptron Networks in Pytorch with CUDA acceleration.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-pytorch-mlpc-gpu-training&#34;&gt;ML-python-pytorch-mlpc-gpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-pytorch-mlpc-gpu-service&#34;&gt;ML-python-pytorch-mlpc-gpu-service&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;img src=&#34;https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png&#34; alt=&#34;Scikit Logo&#34; width=&#34;140&#34; align=&#34;right&#34;&gt;
&lt;h2 id=&#34;scikit-learn&#34;&gt;Scikit-Learn&lt;/h2&gt;
&lt;p&gt;Simple and efficient tools for predictive data analysis, accessible to everybody, and reusable in various contexts.&lt;/p&gt;
&lt;p&gt;Built on NumPy, SciPy, and matplotlib&lt;/p&gt;
&lt;p&gt;Documentation is at: &lt;a href=&#34;https://scikit-learn.org/&#34;&gt;https://scikit-learn.org/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;k-nearest-neighbor-classification&#34;&gt;K Nearest Neighbor Classification&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-sklearn-knc-cpu&lt;/code&gt; is a project for training and deploying K Nearest Neighbor Classification using the SciKit-Learn library.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-knc-cpu-training&#34;&gt;ML-python-sklearn-knc-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-knc-cpu-service&#34;&gt;ML-python-sklearn-knc-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;naive-bayes-classification&#34;&gt;Naive Bayes Classification&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-sklearn-nbc-cpu&lt;/code&gt; is a project for training and deploying Naive Bayes Classification using the SciKit-Learn library.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-nbc-cpu-training&#34;&gt;ML-python-sklearn-nbc-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-nbc-cpu-service&#34;&gt;ML-python-sklearn-nbc-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;random-forest-classification&#34;&gt;Random Forest Classification&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-sklearn-rfc-cpu&lt;/code&gt; is a project for training and deploying Random Forest Classifications using the SciKit-Learn library&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-rfc-cpu-training&#34;&gt;ML-python-sklearn-rfc-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-rfc-cpu-service&#34;&gt;ML-python-sklearn-rfc-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;ridge-classification&#34;&gt;Ridge Classification&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-sklearn-rc-cpu&lt;/code&gt; is a project for training and deploying Random Forest Classification using the SciKit-Learn library.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-rc-cpu-training&#34;&gt;ML-python-sklearn-rc-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-rc-cpu-service&#34;&gt;ML-python-sklearn-rc-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;support-vector-machines&#34;&gt;Support Vector Machines&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-sklearn-svm-cpu&lt;/code&gt; is a project for training and deploying Support Vector Machines using the SciKit-Learn library.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-svm-cpu-training&#34;&gt;ML-python-sklearn-svm-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-sklearn-svm-cpu-service&#34;&gt;ML-python-sklearn-svm-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;img src=&#34;https://www.gstatic.com/devrel-devsite/prod/vbc166ea82921a0c6d4f6ee6c94a3e0bcf7b885b334dd31c4592509cb25134992/tensorflow/images/lockup.svg&#34; alt=&#34;TensorFlow Logo&#34; width=&#34;140&#34; align=&#34;right&#34;&gt;
&lt;h2 id=&#34;tensorflow&#34;&gt;TensorFlow&lt;/h2&gt;
&lt;p&gt;TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.&lt;/p&gt;
&lt;p&gt;Documentation is at &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;https://www.tensorflow.org/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;classification-mnist&#34;&gt;Classification (MNIST)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-tensorflow-mnist-cpu&lt;/code&gt; is a project for training and deploying an MNIST classifier using TensorFlow.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-tensorflow-mnist-cpu-training&#34;&gt;ML-python-tensorflow-mnist-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-tensorflow-mnist-cpu-service&#34;&gt;ML-python-tensorflow-mnist-cpu-service&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-tensorflow-mnist-gpu&lt;/code&gt; is a project for training and deploying an MNIST classifier using TensorFlow with CUDA acceleration.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-tensorflow-mnist-gpu-training&#34;&gt;ML-python-tensorflow-mnist-gpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-tensorflow-mnist-gpu-service&#34;&gt;ML-python-tensorflow-mnist-gpu-service&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;img src=&#34;https://xgboost.ai/images/logo/xgboost-logo.png&#34; alt=&#34;XGBoost Logo&#34; width=&#34;140&#34; align=&#34;right&#34;&gt;
&lt;h2 id=&#34;xgboost&#34;&gt;XGBoost&lt;/h2&gt;
&lt;p&gt;Scalable and flexible Gradient Boosting. Supports regression, classification, ranking and user defined objectives.&lt;/p&gt;
&lt;p&gt;Documentation is at: &lt;a href=&#34;https://xgboost.readthedocs.io/en/latest/&#34;&gt;https://xgboost.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;gradient-boosted-decision-trees&#34;&gt;Gradient Boosted Decision Trees&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CPU-based:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ML-python-xgb-cpu&lt;/code&gt; is a project for training and deploying gradient boosted decision trees using the XGBoost library.&lt;/p&gt;
&lt;p&gt;Training Script : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-xgb-cpu-training&#34;&gt;ML-python-xgb-cpu-training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Service Wrapper : &lt;a href=&#34;https://github.com/machine-learning-quickstarts/ML-python-xgb-cpu-service&#34;&gt;ML-python-xgb-cpu-service&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Working with GPUs</title>
      <link>https://jenkins-x.io/docs/resources/guides/mlops/gpu/</link>
      <pubDate>Fri, 13 Mar 2020 15:03:05 +0000</pubDate>
      
      <guid>https://jenkins-x.io/docs/resources/guides/mlops/gpu/</guid>
      <description>
        
        
        &lt;p&gt;To use CUDA to accelerate your ML training and services, you first need to set up your Kubernetes cluster to add some physical GPU resources to your nodes.&lt;/p&gt;
&lt;p&gt;To do this, typically you will need to request an allocation of GPU resources from your Cloud provider and then configure an additional Node Pool to provision a set of Nodes such that each Node has access to at least one physical GPU card.&lt;/p&gt;
&lt;p&gt;See the documentation for your Cloud platform for details.
The instructions for GCP are here: &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/gpus&#34;&gt;https://cloud.google.com/kubernetes-engine/docs/how-to/gpus&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Additionally, each Node will require a CUDA installation and the drivers appropriate to the physical accelerator cards chosen. Note that this introduces fixed dependencies upon the driver version for your specific hardware and the installed CUDA version that spans the entire Node Pool, impacting every service you deploy to this pool.&lt;/p&gt;
&lt;p&gt;Be aware that you will be charged for use of the GPU resources from the point at which each Node starts up, NOT just whilst you are running a training! It is recommended to use elastic node scaling on your pool so that you release GPU resources that are not currently being utilised.&lt;/p&gt;
&lt;p&gt;Once your cluster is configured, you can allocate GPU resources to containers as part of your application config.&lt;/p&gt;
&lt;p&gt;For the training project, you will need to ensure that the build container used has access to GPU resources. This can be provisioned via the &lt;code&gt;jenkins-x.yml&lt;/code&gt; file in that project, like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;buildPack: ml-python-gpu-training

pipelineConfig:
  pipelines:
    overrides:
      - pipeline: release
        stage: training
        name: training
        containerOptions:
          resources:
            limits:
              cpu: 4
              memory: 32Gi
              nvidia.com/gpu: 1
            requests:
              cpu: 0.5
              memory: 8Gi
              nvidia.com/gpu: 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that at the moment, it is not possible to modify the container resources of a single pipeline step, but only the resources for every container in a stage. As a result, it is necessary to perform all ML build activities in a single step in a dedicated stage or Kubernetes will attempt to allocate a physical GPU to a new container for every step in the stage, draining all available resources and likely blocking the build.&lt;/p&gt;
&lt;p&gt;This pipeline config is set up for you in all the existing GPU quickstarts.&lt;/p&gt;
&lt;p&gt;To configure the service project, you can adjust the resource section of the &lt;code&gt;values.yaml&lt;/code&gt; file in the project chart to set &lt;code&gt;nvidia.com/gpu: 2&lt;/code&gt; to indicate how many GPU cards to allocate to each Pod instance. Note that you cannot exceed the number of GPUs available to a Node and Pods may become unschedulable if there are insufficient free GPU cards in the pool.&lt;/p&gt;
&lt;p&gt;Once you have deployed a GPU-based service, it will reserve the cards allocated to it, so care is needed to avoid running up unnecessarily large bills by leaving non-essential services or preview environments up.&lt;/p&gt;
&lt;p&gt;You will, of course, have to ensure that your training script and service implementation code are set up to use CUDA features.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Managing Data</title>
      <link>https://jenkins-x.io/docs/resources/guides/mlops/data/</link>
      <pubDate>Fri, 13 Mar 2020 15:03:05 +0000</pubDate>
      
      <guid>https://jenkins-x.io/docs/resources/guides/mlops/data/</guid>
      <description>
        
        
        &lt;p&gt;In most scenarios, you will be expecting to remotely access data hosted elsewhere on your network and can manage this in code as part of your training scripts and service implementations. There are however a couple of situations in which Jenkins X can help you to manage certain types of data.&lt;/p&gt;
&lt;h2 id=&#34;handling-data-in-buckets&#34;&gt;Handling data in Buckets&lt;/h2&gt;
&lt;p&gt;If you are working with data in the form of arbitrary files, you can transfer these to your training environment via a Storage Bucket in your Cloud project with the following helper step within your build pipeline:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; jx step unstash -u &amp;lt;URL of bucket&amp;gt; -o &amp;lt;output filename or directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The URL should be in the form: s3://mybucket/tests/myOrg/myData/trainingset.xml where the protocol is set as appropriate to the Cloud provider you are using.&lt;/p&gt;
&lt;p&gt;You must ensure that the data has been uploaded to this bucket prior to starting the training build and should bear in mind that this command copies the specified file from the bucket to the working volume of the build container executing the current build step.&lt;/p&gt;
&lt;h2 id=&#34;working-with-volumes&#34;&gt;Working with Volumes&lt;/h2&gt;
&lt;p&gt;Under some circumstances, you may wish to create versioned collections of immutable training data that can be shared across multiple models and which are too large to easily copy from buckets in a timely manner.&lt;/p&gt;
&lt;p&gt;Under these circumstances, it is straightforward to create a named, persistent Kubernetes Volume within your Cloud project, mount it in read/write mode and upload your training data files to it, then unmount it ready for use.&lt;/p&gt;
&lt;p&gt;Within your training pipelines, you can then specify that this volume be mounted during the training build, in read-only mode, in more than one project in parallel.&lt;/p&gt;
&lt;p&gt;To do this, you need to modify the &lt;code&gt;jenkins-x.yml&lt;/code&gt; file in your training projects to reference the &lt;code&gt;volume&lt;/code&gt; and &lt;code&gt;volumeMount&lt;/code&gt; config necessary to connect the build container instance to your training data volume.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;buildPack: ml-python-gpu-training

pipelineConfig:
  pipelines:
    overrides:
      - pipeline: release
        stage: training
        name: training
        volumes:
          - name: trainingset
            gcePersistentDisk:
              pdName: mytrainingvolume01
              fsType: ext4
              readonly: true
        containerOptions:
          resources:
            limits:
              cpu: 4
              memory: 32Gi
              nvidia.com/gpu: 1
            requests:
              cpu: 0.5
              memory: 8Gi
              nvidia.com/gpu: 1
          volumeMounts:
            - name: trainingset
              mountPath: /trainingset
              readOnly: true 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that it is only possible to simultaneously share volumes that are mounted read-only.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
